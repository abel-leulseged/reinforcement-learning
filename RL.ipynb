{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import operator\n",
    "# suppress scientific notation printing in numpy \n",
    "np.set_printoptions(suppress=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = np.array([\"^\", \">\", \"v\", \"<\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a dictionary that stores the rewards for all the actions in every possible state\n",
    "moves = {\n",
    "  \"0\": { \"^\": [\"0\", -10], \"v\": [\"3\", -1], \"<\": [\"0\", -10], \">\": [\"1\", -1] },\n",
    "  \"1\": { \"^\": [\"1\", -10], \"v\": [\"4\", -1], \"<\": [\"0\", -1],  \">\": [\"2\", -1] },\n",
    "  \"2\": {\"^\":  [\"2\", -10], \"v\": [\"5\", -1], \"<\": [\"1\", -1],  \">\": [\"2\", -10]},\n",
    "  \"3\": {\"^\":  [\"0\", -1],  \"v\": [\"6\", -1], \"<\": [\"3\", -10], \">\": [\"4\", -1] },\n",
    "  \"4\": {\"^\":  [\"1\", -1],  \"v\": [\"7\", -1], \"<\": [\"3\", -1],  \">\": [\"5\", -1] },\n",
    "  \"5\": {\"^\":  [\"2\", -1],  \"v\": [\"8\", -1], \"<\": [\"4\", -1],  \">\": [\"5\", -10]},\n",
    "  \"6\": {\"^\":  [\"3\", -1],  \"v\": [\"6\", -10],\"<\": [\"6\", -10], \">\": [\"7\", -1] },\n",
    "  \"7\": {\"^\":  [\"4\", -1],  \"v\": [\"7\", -10],\"<\": [\"6\", -1],  \">\": [\"8\", -1] },\n",
    "  \"8\": {\"^\":  [\"5\", -1],  \"v\": [\"8\", -10],\"<\": [\"7\", -1],  \">\": [\"8\", -10]}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# each row here is a state and the column correspond to actions as they are ordered in the above actions array\n",
    "policy = np.ones((9,4))*0.25\n",
    "policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-9999., -9999., -9999., -9999.],\n",
       "       [-9999., -9999., -9999., -9999.],\n",
       "       [-9999., -9999., -9999., -9999.],\n",
       "       [-9999., -9999., -9999., -9999.],\n",
       "       [-9999., -9999., -9999., -9999.],\n",
       "       [-9999., -9999., -9999., -9999.],\n",
       "       [-9999., -9999., -9999., -9999.],\n",
       "       [-9999., -9999., -9999., -9999.],\n",
       "       [-9999., -9999., -9999., -9999.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# each row here is a state and the column correspond to actions as they are ordered in the above actions array\n",
    "Q = np.ones((9,4))*-9999\n",
    "Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['0', '1', '2', '3', '4', '5', '6', '7'], dtype='<U21')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_states = np.arange(8).astype(str)\n",
    "start_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['8'], dtype='<U21')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_state = np.array([8]).astype(str)\n",
    "final_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_prob = np.array([0.125 for i in start_states])\n",
    "start_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 0), (0, 0), (0, 0), (0, 0)],\n",
       " [(0, 0), (0, 0), (0, 0), (0, 0)],\n",
       " [(0, 0), (0, 0), (0, 0), (0, 0)],\n",
       " [(0, 0), (0, 0), (0, 0), (0, 0)],\n",
       " [(0, 0), (0, 0), (0, 0), (0, 0)],\n",
       " [(0, 0), (0, 0), (0, 0), (0, 0)],\n",
       " [(0, 0), (0, 0), (0, 0), (0, 0)],\n",
       " [(0, 0), (0, 0), (0, 0), (0, 0)],\n",
       " [(0, 0), (0, 0), (0, 0), (0, 0)]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# each row here is a state and the column correspond to actions as they are ordered in the above actions array\n",
    "# each entry is a tuple where the first element of the tuple is the sums so far and the second element of\n",
    "# the tuple is the number of sums far\n",
    "rewards = [[(0,0)for action in range(4)] for state in range(9)]\n",
    "rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def action_offset(action):\n",
    "    \"\"\"takes your requested action and adds an offset.\n",
    "       80% probability for 0 offset, 10% chance for left offset, 10% chance for right offset\n",
    "       \"\"\"    \n",
    "    offset = np.random.choice([-1,0,1], 1, p=[0.1,0.8,0.1])\n",
    "    return actions[ (int(np.where(actions == action)[0]) + offset) % len(actions)][0]   \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transition(state, policy):\n",
    "    \"\"\" simulates the transition from the input state using the input poicy and \n",
    "        returns reward, new_state, requested_action, actual_action\n",
    "    \"\"\"\n",
    "    requested_action = np.random.choice( actions, 1, p=policy[int(state)])\n",
    "    actual_action = action_offset(requested_action)\n",
    "    new_state, reward = moves[state][actual_action]\n",
    "    return reward, new_state, requested_action[0], actual_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_episode(state, policy, actions, print_flag=True):\n",
    "    \"\"\" simuates a single episode starting from the input state and using the input policy \n",
    "        until it reaches the final state. It returns the trace for the episode and the rewards \n",
    "        at each line of the episode\n",
    "    \"\"\"\n",
    "    trace = []\n",
    "    rewards_list = []\n",
    "    # first randomly chosen state action pair\n",
    "    requested_action = random.choice(actions)\n",
    "    actual_action = action_offset(requested_action)\n",
    "    new_state, reward = moves[state][actual_action]\n",
    "    if print_flag:\n",
    "        print(\"Starting in State \", state)\n",
    "        state = new_state\n",
    "        print((reward, new_state, requested_action, actual_action))\n",
    "    while state != final_state[0]:\n",
    "        reward, new_state, requested_action, actual_action = transition(state, policy)\n",
    "        if print_flag:\n",
    "            print((reward, new_state, requested_action, actual_action))\n",
    "        trace.append((reward, state, requested_action, actual_action))\n",
    "        rewards_list.append(reward)\n",
    "        state = new_state\n",
    "    trace.append((reward, state, requested_action, actual_action))\n",
    "    if print_flag:\n",
    "        print(\" FINAL STATE REACHED!!!\")\n",
    "    return trace, rewards_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def G(trace, rewards_list, line):\n",
    "    \"\"\" sum all the rewards after the first occurence of input line in the input list\n",
    "        returns the number of rewards summed and total sum of rewards\n",
    "    \"\"\"\n",
    "    first_occur = trace.index(line)+1\n",
    "    after_first_occur = rewards_list[first_occur:]\n",
    "    return sum(after_first_occur)\n",
    "#     return (sum(after_first_occur), len(after_first_occur))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accumulate_rewards(rewards, trace, rewards_list):\n",
    "    \"\"\" populates the previously empty rewards matrix with the rewards of the state-action pairs in the input trace \n",
    "    \"\"\"\n",
    "    # this first loop gets the rewards for each state action pair into a dictionary to avoid duplicates\n",
    "    state_action_pair_reward = {}\n",
    "    for line in trace:      \n",
    "        state_action_pair = (int(line[1]), int(np.where(actions==line[2])[0]))                            \n",
    "        if state_action_pair not in state_action_pair_reward:\n",
    "            state_action_pair_reward[state_action_pair] = G(trace, rewards_list, line)\n",
    "\n",
    "    # this second loop populates the rewards matrix using the dictionary\n",
    "    for state_action_pair in state_action_pair_reward:\n",
    "        state, action = state_action_pair\n",
    "        rewards[state][action] = tuple(map(operator.add,rewards[state][action],\n",
    "                                           (state_action_pair_reward[state_action_pair],1)))\n",
    "#         rewards[state][action] = tuple(map(operator.add,rewards[state][action],\n",
    "#                                            state_action_pair_reward[state_action_pair]))\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateQ(Q, rewards):\n",
    "    \"\"\" updates and returns Q based on the input trace\n",
    "    \"\"\"    \n",
    "    # this loop replaces each entry of Q with the average of the corresponding non-empty list entry in \n",
    "    # rewards matrix\n",
    "    for state in range(len(rewards)):\n",
    "        for action in range(len((actions))):\n",
    "            if rewards[state][action][1] !=0:\n",
    "                Q[state][action] = rewards[state][action][0]/rewards[state][action][1]\n",
    "    return Q    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updatePolicy(policy, Q):\n",
    "    \"\"\" updates and returns the input policy matrix based on the input Q matrix\n",
    "    \"\"\"\n",
    "    for state in range(len(policy)):\n",
    "        optimal_action = np.argmax(Q[state])\n",
    "        # create a boolean mask to replace the non-optimal action probailities with 0 and the optimal probaility with 1\n",
    "        mask = np.zeros((1,4),dtype=bool)[0]\n",
    "        mask[optimal_action] = True\n",
    "        # assign optimal action a probablity of 1 and all other actions a probability of 0\n",
    "        policy[state][mask] = 1 \n",
    "        policy[state][~mask] = 0\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(policy):\n",
    "    \"\"\" takes in a 9x4 matrix of 0's and 1's where rows correspond to states and columns correspond to actions\n",
    "        returns a 3x3 matrix containing the policy's suggested actions at each cell\n",
    "    \"\"\"\n",
    "    visualized = np.zeros((3,3)).astype(str)\n",
    "    rows = visualized.shape[0]\n",
    "    cols = visualized.shape[1]\n",
    "    for row in range(rows):\n",
    "        for col in range(cols):\n",
    "            visualized[row][col] = actions[np.argmax(policy[row*rows+col])]\n",
    "    visualized[row][col] = 'T'\n",
    "    return visualized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo Method with Exploring Starts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MCMES1(policy, Q, rewards, num_episodes):\n",
    "    \"\"\" \n",
    "        inputs: policy, Q and rewards are the initialized policy, Q and rewards matrix\n",
    "        This functions uses all the above initializations and helper functions to simulate MCM with exploring starts.\n",
    "        After every num_episodes episodes, we update the policy. We keep doing this until we get the optimal\n",
    "        policy\n",
    "    \"\"\"\n",
    "    convergence_counter = 0\n",
    "    iteration = 0\n",
    "    print((\"-\"*20+\"ITERATION {}\"+\"-\"*20).format(iteration))\n",
    "    while True:\n",
    "        # if we have yet to see to consecutive episodes with the same policy\n",
    "        for episode in range(num_episodes):\n",
    "            start_state = np.random.choice(start_states)\n",
    "            print((\"-\"*20+\"EPISODE {}\"+\"-\"*20).format(episode))\n",
    "            trace, rewards_list = one_episode(start_state, policy, actions)\n",
    "            rewards = accumulate_rewards(rewards, trace, rewards_list)\n",
    "        Q = updateQ(Q, rewards)\n",
    "        print('-'*60)\n",
    "        print(\"Q for iteration {}:\".format(iteration))\n",
    "        print(Q[:8])\n",
    "        previous_policy = policy\n",
    "        policy = updatePolicy(policy, Q)\n",
    "        print(\"POLICY for iteration {}:\".format(iteration))\n",
    "        print(visualize(policy))\n",
    "        print()\n",
    "        iteration += 1\n",
    "        if (policy - previous_policy == np.zeros((9,4))).all():\n",
    "            # check if the policy changed \n",
    "            convergence_counter += 1\n",
    "        else:\n",
    "            # once Q changes, reset counter\n",
    "            convergence_counter = 0 \n",
    "        if convergence_counter == 1:\n",
    "            # if policy hasn't changed in the last 10 iterations, we assume it has converged to the optimal policy\n",
    "            print(\"OPTIMAL POLICY IS\")\n",
    "            print(visualize(policy))\n",
    "            break \n",
    "        print((\"-\"*20+\"ITERATION {}\"+\"-\"*20).format(iteration))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I chose to evaluate and update the policy every 20 episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# MCMES1(policy, Q, rewards, 2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MCMES2(policy, Q, rewards, num_episodes):\n",
    "    \"\"\" \n",
    "        inputs: policy, Q and rewards are the initialized policy, Q and rewards matrix\n",
    "        This functions uses all the above initializations and helper functions to simulate MCM with exploring starts.\n",
    "        After every num_episodes episodes, we update the policy. We keep doing this until our Q converges.\n",
    "        Convergence here is defined as 10 iterations without any change in Q.\n",
    "    \"\"\"\n",
    "    convergence_counter = 0\n",
    "    iteration = 0\n",
    "    print((\"-\"*20+\"ITERATION {}\"+\"-\"*20).format(iteration))\n",
    "    episodes = 0\n",
    "    while True:\n",
    "        # if we have yet to see to consecutive episodes with the same policy\n",
    "        for episode in range(num_episodes):\n",
    "            start_state = np.random.choice(start_states)\n",
    "            trace, rewards_list = one_episode(start_state, policy, actions, print_flag=False)\n",
    "            rewards = accumulate_rewards(rewards, trace, rewards_list)\n",
    "            episodes += 1\n",
    "        previous_Q = Q\n",
    "        Q = updateQ(Q, rewards)\n",
    "        print(\"Q for iteration {}:\".format(iteration))\n",
    "        print(Q[:8])\n",
    "        policy = updatePolicy(policy, Q)\n",
    "        print(\"POLICY for iteration {}:\".format(iteration))\n",
    "        print(visualize(policy))\n",
    "        print()\n",
    "        iteration += 1\n",
    "        if (Q - previous_Q == np.zeros((9,4))).all():\n",
    "            # check if the Q changed \n",
    "            convergence_counter += 1\n",
    "        else:\n",
    "            # once Q changes, reset counter\n",
    "            convergence_counter = 0 \n",
    "        if convergence_counter == 5:\n",
    "            print(\"Q converges after {0} iteration(s) and {1} episodes\".format(iteration,episodes))\n",
    "            print(\"FINAL Q IS\")\n",
    "            print(Q[:8])\n",
    "            print(\"FINAL POLICY IS\")\n",
    "            print(visualize(policy))\n",
    "            break\n",
    "        print((\"-\"*20+\"ITERATION {}\"+\"-\"*20).format(iteration))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, I chose to evaluate and update the policy every 100 episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = [[(0,0)for action in range(4)] for state in range(9)]\n",
    "policy = np.ones((9,4))*0.25\n",
    "Q = np.ones((9,4))*-9999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------ITERATION 0--------------------\n",
      "Q for iteration 0:\n",
      "[[-111.53665689  -97.69586375 -100.70301624 -107.53693182]\n",
      " [ -99.88980716  -94.08232446  -83.40045767 -113.55927835]\n",
      " [ -88.4939759   -89.2808642   -69.16666667  -96.37340153]\n",
      " [-105.37864078  -88.22374429  -96.28368794 -101.19788918]\n",
      " [ -98.8         -71.39812646  -65.89732143  -96.78894472]\n",
      " [ -88.37846154  -59.0798722   -13.45102506  -76.42735043]\n",
      " [ -96.38944724  -72.63157895  -97.01904762  -98.11180124]\n",
      " [ -77.09243697  -13.5950783   -63.69306931  -88.67192429]]\n",
      "POLICY for iteration 0:\n",
      "[['>' 'v' 'v']\n",
      " ['>' 'v' 'v']\n",
      " ['>' '>' 'T']]\n",
      "\n",
      "--------------------ITERATION 1--------------------\n",
      "Q for iteration 1:\n",
      "[[-111.53665689  -71.91929825 -100.70301624 -107.53693182]\n",
      " [ -99.88980716  -94.08232446  -51.51432469 -113.55927835]\n",
      " [ -88.4939759   -89.2808642   -51.76897133  -96.37340153]\n",
      " [-105.37864078  -60.45496183  -96.28368794 -101.19788918]\n",
      " [ -98.8         -71.39812646  -30.88270378  -96.78894472]\n",
      " [ -88.37846154  -59.0798722    -8.20350404  -76.42735043]\n",
      " [ -96.38944724  -56.32541133  -97.01904762  -98.11180124]\n",
      " [ -77.09243697   -5.62180579  -63.69306931  -88.67192429]]\n",
      "POLICY for iteration 1:\n",
      "[['>' 'v' 'v']\n",
      " ['>' 'v' 'v']\n",
      " ['>' '>' 'T']]\n",
      "\n",
      "--------------------ITERATION 2--------------------\n",
      "Q for iteration 2:\n",
      "[[-111.53665689  -57.40027322 -100.70301624 -107.53693182]\n",
      " [ -99.88980716  -94.08232446  -38.95695696 -113.55927835]\n",
      " [ -88.4939759   -89.2808642   -42.00404313  -96.37340153]\n",
      " [-105.37864078  -47.68669022  -96.28368794 -101.19788918]\n",
      " [ -98.8         -71.39812646  -21.11010363  -96.78894472]\n",
      " [ -88.37846154  -59.0798722    -5.92235734  -76.42735043]\n",
      " [ -96.38944724  -45.81991215  -97.01904762  -98.11180124]\n",
      " [ -77.09243697   -3.75238095  -63.69306931  -88.67192429]]\n",
      "POLICY for iteration 2:\n",
      "[['>' 'v' 'v']\n",
      " ['>' 'v' 'v']\n",
      " ['>' '>' 'T']]\n",
      "\n",
      "--------------------ITERATION 3--------------------\n",
      "Q for iteration 3:\n",
      "[[-111.53665689  -46.80087051 -100.70301624 -107.53693182]\n",
      " [ -99.88980716  -94.08232446  -31.0007722  -113.55927835]\n",
      " [ -88.4939759   -89.2808642   -35.5524239   -96.37340153]\n",
      " [-105.37864078  -39.60690316  -96.28368794 -101.19788918]\n",
      " [ -98.8         -71.39812646  -16.21784528  -96.78894472]\n",
      " [ -88.37846154  -59.0798722    -4.68668596  -76.42735043]\n",
      " [ -96.38944724  -38.97170972  -97.01904762  -98.11180124]\n",
      " [ -77.09243697   -2.91195694  -63.69306931  -88.67192429]]\n",
      "POLICY for iteration 3:\n",
      "[['>' 'v' 'v']\n",
      " ['>' 'v' 'v']\n",
      " ['>' '>' 'T']]\n",
      "\n",
      "--------------------ITERATION 4--------------------\n",
      "Q for iteration 4:\n",
      "[[-111.53665689  -40.81209302 -100.70301624 -107.53693182]\n",
      " [ -99.88980716  -94.08232446  -26.46577095 -113.55927835]\n",
      " [ -88.4939759   -89.2808642   -30.46723647  -96.37340153]\n",
      " [-105.37864078  -34.29292107  -96.28368794 -101.19788918]\n",
      " [ -98.8         -71.39812646  -13.5833968   -96.78894472]\n",
      " [ -88.37846154  -59.0798722    -3.82825962  -76.42735043]\n",
      " [ -96.38944724  -33.98839662  -97.01904762  -98.11180124]\n",
      " [ -77.09243697   -2.46203111  -63.69306931  -88.67192429]]\n",
      "POLICY for iteration 4:\n",
      "[['>' 'v' 'v']\n",
      " ['>' 'v' 'v']\n",
      " ['>' '>' 'T']]\n",
      "\n",
      "Q converges after 5 iteration(s) and 5000 episodes\n",
      "FINAL Q IS\n",
      "[[-111.53665689  -40.81209302 -100.70301624 -107.53693182]\n",
      " [ -99.88980716  -94.08232446  -26.46577095 -113.55927835]\n",
      " [ -88.4939759   -89.2808642   -30.46723647  -96.37340153]\n",
      " [-105.37864078  -34.29292107  -96.28368794 -101.19788918]\n",
      " [ -98.8         -71.39812646  -13.5833968   -96.78894472]\n",
      " [ -88.37846154  -59.0798722    -3.82825962  -76.42735043]\n",
      " [ -96.38944724  -33.98839662  -97.01904762  -98.11180124]\n",
      " [ -77.09243697   -2.46203111  -63.69306931  -88.67192429]]\n",
      "FINAL POLICY IS\n",
      "[['>' 'v' 'v']\n",
      " ['>' 'v' 'v']\n",
      " ['>' '>' 'T']]\n"
     ]
    }
   ],
   "source": [
    "MCMES2(policy, Q, rewards,1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
